# Predicting Vulnerability-Proneness in GitHub Actions using Contextual Metadata**  

## ğŸ“Œ **Overview**  
This repository provides a **complete pipeline for extracting, processing, and modeling the vulnerability-proneness of GitHub Actions** based on contextual metadata from GitHub repositories. The project directly addresses **RQ3** from the study *Shadows on the Spotless Workflow: The Vulnerability Proneness of GitHub Actions*, investigating how repository-level metadata (e.g., repository activity, contributor engagement, and update frequency) can predict security risks in third-party Actions.  

## ğŸš€ **Pipeline Overview**  
The project follows a structured pipeline to **predict the vulnerability-proneness of GitHub Actions** using multiple data processing steps:

### **1ï¸âƒ£ Data Extraction & Processing (`src/`)**  
- Extracts repository metadata from GitHub API (`github_api.py`).  
- Cleans and structures raw repository data (`data_manager.py`).  
- Computes various repository metrics (`repo_metrics.py`).  
- Processes dependencies and security indicators (`dependency_analyzer.py`).  
- Generates the **layered datasets (`datos_capa_0.csv` to `datos_capa_7.csv`)**.  

### **2ï¸âƒ£ Feature Engineering & Model Training (`docs/experimental_setup/`)**  
- **Feature selection & transformation** (`02_feature_engineering.ipynb`).  
- **Training machine learning models** to predict vulnerability-proneness (`03_model_training.ipynb`).  
- Evaluates model performance using **Random Forest, XGBoost**, and other classifiers.  

## ğŸ”§ **Installation & Usage**
### **1ï¸âƒ£ Install dependencies**
```bash
pip install -r requirements.txt
```

### **2ï¸âƒ£ Run the data processing pipeline**
```bash
python main.py
```
This will **extract repository metadata, process dependencies, and generate all processed datasets** in `data/processed/`.  

### **3ï¸âƒ£ View Results**  
- All outputs from the pipeline, including processed datasets and model evaluation metrics, are stored as **text and CSV files**.  
- The final results can be found in **`docs/experimental_setup/datasets/`**, including:  
  - **Processed datasets** (`final_cv_results.csv`, `final_xgb_results.csv`).  
  - **Model performance metrics** (`final_cv_results_all_metrics.csv`).  
  - **Trained machine learning models** (`RandomForest.joblib`, `XGBoost.joblib`).  

### **4ï¸âƒ£ Optional: Run the Notebooks for Further Analysis**  
If you want to explore or modify the feature engineering and modeling steps, navigate to `docs/experimental_setup/` and execute:  
```bash
jupyter notebook
```
Then open and run the following:  
- `02_feature_engineering.ipynb` (Feature Engineering)  
- `03_model_training.ipynb` (Model Training & Evaluation)  

While **running the notebooks is optional**, they allow you to inspect and modify the **feature engineering and model training process**.  

## ğŸ“Š **Experiment & Results**
- The experiment evaluates **how well contextual metadata predicts vulnerability risks**.  
- Trained models are stored in **`docs/experimental_setup/models/`** (Random Forest, XGBoost, etc.).  
- Key results and performance comparisons are available in **CSV and text format**.  

---

## ğŸ“‚ **Repository Structure**
```
pavt-vp-data-extraction/
â”œâ”€â”€ main.py                        # Entry point for data extraction & processing
â”œâ”€â”€ requirements.txt               # Project dependencies
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # Raw repository data
â”‚   â”‚   â”œâ”€â”€ repositories_raw_data.csv
â”‚   â”‚   â”œâ”€â”€ repositories_raw_data_test.csv
â”‚   â”œâ”€â”€ processed/                  # Processed datasets by layers
â”‚   â”‚   â”œâ”€â”€ datos_capa_0.csv
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ datos_capa_7.csv
â”œâ”€â”€ docs/                          # Documentation & experiment results
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ rq3-approach.md             # Methodology for RQ3
â”‚   â”œâ”€â”€ vp-distribution.ipynb       # Vulnerability-proneness analysis
â”‚   â”œâ”€â”€ experimental_setup/         # ML model training & evaluation
â”‚   â”‚   â”œâ”€â”€ 01_data_preprocessing.ipynb
â”‚   â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â”‚   â”œâ”€â”€ 03_model_training.ipynb
â”‚   â”‚   â”œâ”€â”€ datasets/               # Processed datasets & trained models
â”‚   â”‚   â”‚   â”œâ”€â”€ final_cv_results.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ final_xgb_results.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ RandomForest.joblib
â”‚   â”‚   â”‚   â”œâ”€â”€ XGBoost.joblib
â”œâ”€â”€ src/                           # Source code for data extraction & processing
â”‚   â”œâ”€â”€ core/                      # Core pipeline components
â”‚   â”‚   â”œâ”€â”€ pipeline_manager.py
â”‚   â”‚   â”œâ”€â”€ github_api.py
â”‚   â”œâ”€â”€ processing/                 # Data processing & dependency analysis
â”‚   â”‚   â”œâ”€â”€ dependency_analyzer.py
â”‚   â”‚   â”œâ”€â”€ language_processor.py
â”‚   â”‚   â”œâ”€â”€ pipeline_steps/
â”‚   â”‚   â”‚   â”œâ”€â”€ feature_engineering_step.py
â”‚   â”œâ”€â”€ utils/                      # Utility scripts
â”‚   â”‚   â”œâ”€â”€ repo_metrics.py
â”‚   â”‚   â”œâ”€â”€ transform_data.py
```

This repository provides a **complete framework for predicting the vulnerability-proneness of GitHub Actions using contextual metadata**, supporting the findings of *Shadows on the Spotless Workflow: The Vulnerability Proneness of GitHub Actions*.